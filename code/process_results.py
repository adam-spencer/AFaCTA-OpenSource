"""
Combine multiple results files generated by `afacta_multi_step_annotation.py`
into a multi-model, multi-origin results file.

Also processes select columns such that the `compute_score.py` script can
handle them properly.

Author:
    Adam Spencer
"""
import argparse
from collections import defaultdict
from functools import reduce
import pandas as pd
from pathlib import Path
import numpy as np


# ripped from compute_scores_updated.py
def compute_likelihood(df_to_eval, model_names):
    for model in model_names:
        # Skip incorrectly formatted cols (original llama and zephyr)
        if f'{model}-s1' not in df_to_eval.columns:
            continue
        p1 = df_to_eval[f'{model}-s1'].apply(
            lambda x: 1 if "yes" in x.lower() else 0).values
        p2 = []
        for i, c in zip(df_to_eval[f'{model}-s2'],
                        df_to_eval[f'{model}-category']):
            if i or 'C0' not in c:
                p2.append(1)
            else:
                p2.append(0)
        p2 = np.array(p2)
        p3_1 = df_to_eval[f'{model}-s3-1'].apply(
            lambda x: 1 if "objective" in x.lower() else 0).values
        p3_2 = df_to_eval[f'{model}-s3-2'].apply(
            lambda x: 1 if "objective" in x.lower() else 0).values

        df_to_eval[model] = p1 + p2 + 0.5 * p3_1 + 0.5 * p3_2
    return df_to_eval


def rename_and_filter_model_cols(df: pd.DataFrame, model_name: str
                                 ) -> pd.DataFrame:
    """
    Rename select columns to work with `compute_scores.py` script and change
    others to be linked to the model they are output from.
    """
    rename_dict = {
        'veri_aggregated': f'{model_name}-s1',
        'p2_aggregated': f'{model_name}-s2',
        'CATEGORY': f'{model_name}-category',
        'ob_aggregated': f'{model_name}-s3-1',
        'sub_aggregated': f'{model_name}-s3-2'
    }
    other_cols = {
        col_name: f'{model_name}-{col_name}' for col_name in
        set(df.columns) - rename_dict.keys()
        if col_name not in {'SENTENCES', 'ORIGIN'}}
    rename_dict |= other_cols

    df = df.rename(columns=rename_dict)
    return df


def bool_s2_cols(df: pd.DataFrame) -> pd.DataFrame:
    """Convert s2 cols to boolean datatype."""
    def val_to_bool(val):
        if not isinstance(val, str):
            return False
        if 'rue' in val.lower():
            return True
        return False
    for col_name in [col for col in df.columns if col.endswith('-s2')]:
        df[col_name] = df[col_name].apply(val_to_bool)
    return df


def merge_result_dfs(dfs: [pd.DataFrame]) -> pd.DataFrame:
    """Merge model dataframes to create one big df"""
    if 'ORIGIN' in dfs[0].columns:
        merge_keys = ['SENTENCES', 'ORIGIN']
    else:
        merge_keys = ['SENTENCES']
    final_df = reduce(lambda left, right: pd.merge(
        left, right, on=merge_keys, how='outer'), dfs)
    return final_df


def load_file(filename: str) -> pd.DataFrame:
    """Load file as df independent of extension."""
    if filename.endswith('xlsx'):
        df = pd.read_excel(filename)
    else:
        df = pd.read_csv(filename, encoding='utf-8')
    return df.drop_duplicates(subset=['SENTENCES'])


def write_file(output_name: str, df: pd.DataFrame) -> None:
    """Write df to file independent of extension."""
    if output_name.endswith('xlsx'):
        df = df.to_excel(output_name, index=False)
    else:
        df = df.to_csv(output_name, index=False)
    return df


def combine_gold_labels(gold_label_file: str, df: pd.DataFrame
                        ) -> pd.DataFrame:
    """Combine combined df with gold label file."""
    gold_df = load_file(gold_label_file)
    if 'ORIGIN' not in gold_df.columns:
        df = df.drop(columns='ORIGIN')
    return merge_result_dfs([gold_df, df])


def main(args):
    # Collect results files by model & origin
    print('start')
    results_files = defaultdict(dict)
    for file in args.results_files:
        origin, model = Path(file).name.rstrip('_1.csv').split('_', 1)
        results_files[model][origin] = file

    # Concatenate results by model
    print('concat results by model')
    model_dfs = []
    for model, origin_dict in results_files.items():
        dfs = []
        # Add origin col
        for origin, filepath in origin_dict.items():
            df = load_file(filepath)
            df.insert(1, 'ORIGIN', origin)
            dfs.append(df)
        # Concatenate dfs for the same model
        df_concat = pd.concat(dfs, ignore_index=True)
        df_concat = rename_and_filter_model_cols(df_concat, model_name=model)
        df_concat = bool_s2_cols(df_concat)
        model_dfs.append(df_concat)

    # Join resultant dataframes
    print('join dfs')
    final_df = merge_result_dfs(model_dfs)
    if args.likelihood:
        print('likelihood...')
        final_df = compute_likelihood(final_df, results_files.keys())
    if args.gold_file:
        print('gold file join...')
        df_gold = combine_gold_labels(args.gold_file, final_df)
        print('write file...')
        write_file(args.output, df_gold)
    else:
        write_file(args.output, final_df)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('results_files', nargs='+',
                        help='Results CSV or Excel files')
    parser.add_argument('--gold-file', '-g',
                        help='File containing gold labels')
    parser.add_argument('--output', '-o', required=True,
                        help='Output file')
    parser.add_argument('--likelihood', '-l', action='store_true',
                        help='Include likelihood calculation in output')
    args = parser.parse_args()
    main(args)
